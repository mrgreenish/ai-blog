---
title: "Model Personalities"
description: "Every model has a personality. Gemini plays it safe, GPT balances helpfulness with restraint, Claude pushes boundaries. Knowing their tendencies changes how you prompt."
story: "You ask two models the same vague question. One asks for clarification. The other rewrites half your app. Same prompt, wildly different instincts."
category: models
order: 5
interactiveTools: [model-tinder, model-picker]
---

Benchmarks tell you what a model *can* do. Personality tells you what it *will* do when you don't spell out every detail. After months of shipping with these models daily, their personalities are as predictable as coworkers.

This matters more than you'd think. Two models with identical benchmark scores can produce wildly different results for the same task — not because one is smarter, but because one interprets ambiguity differently, takes more initiative, or defaults to a different level of caution.

<ModelTinder />

## Gemini: the careful one

Gemini is the model that asks for permission. It's conservative in its interpretations, sticks close to what you asked for, and rarely goes off-script. If you give it an ambiguous task, it's more likely to ask a clarifying question than to make an assumption and run with it.

**Literal-minded.** Gemini does what you say, not what you might have meant. Ask it to "clean up this component" and it'll fix formatting and remove unused imports. It won't restructure the component hierarchy or suggest a different pattern — even when that's what the code actually needs. You have to ask for that explicitly.

**Risk-averse.** When there are multiple valid approaches, Gemini picks the safest one. The most conventional pattern. The approach with the most documentation. This makes it reliable for production work — you rarely get surprised — but it also means you miss out on better solutions that require a judgment call.

**Consistent under pressure.** In long sessions with large contexts, Gemini tends to stay level. It doesn't drift as much as other models. It doesn't suddenly get "creative" with your architecture at the end of a long conversation. For marathon refactoring sessions, this steadiness is worth a lot.

**Where it falls short.** If you need the model to push back on your approach, suggest alternatives, or notice that you're solving the wrong problem — Gemini usually won't. It's a reliable executor, not a thought partner.

## GPT / Codex: the balanced one

GPT (and by extension Codex) sits in the middle. It takes initiative when the situation is clear, defers when it's ambiguous, and generally produces output that feels "sensible" without being surprising. It's the colleague who does solid work and rarely makes you nervous.

**Pragmatic defaults.** GPT tends to pick the approach that most developers would pick. It reads like Stack Overflow's accepted answer — not the most clever solution, but the one that works and that other developers will understand. For team codebases, this is often exactly what you want.

**Measured initiative.** It'll fill in reasonable gaps without asking — default error handling, obvious edge cases, standard patterns — but it stops short of making architectural decisions. It adds the null check but doesn't restructure your data flow. This hit rate of "helpful without overstepping" is high.

**Good at reading the room.** If your prompt is detailed and specific, GPT stays precise. If your prompt is loose, it makes reasonable assumptions but flags them. It adapts its level of initiative to match your level of specificity, which is a surprisingly useful trait.

**Predictable in output format.** GPT is the most consistent at following output format instructions. If you say "respond in JSON" or "only output the code, no explanation", it almost always complies. Other models sometimes can't help themselves and add commentary anyway.

**Where it falls short.** GPT rarely surprises you with insight. It won't say "actually, you should do this differently" unless you specifically ask for a review. It optimizes for giving you what you asked for, not for giving you what you need.

## Claude Sonnet: the proactive one

Claude has opinions, and it's not shy about sharing them. It's the model most likely to push back on your approach, suggest an alternative, notice a bug you didn't ask about, or restructure your code to be "better" — even when you just wanted a simple change.

**Genuinely creative.** Ask Claude to implement a feature and it might suggest a better API surface, point out that your data model will cause problems at scale, or restructure the code in a way you hadn't considered. When it's right — which is often — this is the most valuable thing a model can do. It's a real thought partner.

**Proactive to a fault.** Claude notices things. While implementing a feature, it'll spot an inconsistent naming convention across files, a missing error boundary, a potential race condition in adjacent code. It'll mention these, and it'll often fix them without being asked. In a short session this feels magical. In a long session it's how you end up reviewing a 40-file diff when you asked for a 3-file change.

**Opinionated about code quality.** Claude has strong preferences about code structure, naming, and patterns. It will refactor code to match its taste. Sometimes its taste is better than yours. Sometimes it's just different, and now you're spending time reviewing changes you didn't want. Setting explicit constraints ("do not refactor unless I ask") is essential.

**Excellent at explaining.** When you need to understand something — why code is broken, how a library works, what tradeoff you're making — Claude gives the clearest, most useful explanations. It connects the dots between your specific code and the general principle. This is where the "personality" really shines.

**Where it falls short.** Scope creep. Claude's instinct to be helpful means it expands tasks in ways that feel reasonable in the moment but make output harder to review. The fix is explicit scope constraints in your prompt or `CLAUDE.md`.

## Claude Opus: the deep thinker

Opus is Claude dialed up. Everything that makes Sonnet distinctive — the creativity, the proactiveness, the opinions — Opus has more of. But it adds something Sonnet doesn't quite match: depth. Opus doesn't just notice things; it understands them in context.

**Deep accuracy.** Where other models pattern-match, Opus traces the actual logic. It catches bugs that require understanding three levels of indirection. It identifies race conditions by mentally simulating concurrent execution. It spots type issues that TypeScript itself misses. This isn't just "smarter" — it's a qualitatively different way of engaging with code.

**Architecturally creative.** Opus doesn't just suggest a different function name — it might suggest a different abstraction entirely, and explain why your current approach will create problems two features from now. It thinks in systems, not just in code. When you're making design decisions that will compound, this perspective is worth the cost.

**Thorough to the point of slow.** Opus takes its time. It considers more options, explores more edge cases, and gives more complete answers. For quick tasks this is overkill — you're paying for depth you don't need. For hard problems, architecture decisions, and debugging subtle issues, the thoroughness pays for itself many times over.

**Proactive with substance.** Where Sonnet might fix a naming inconsistency, Opus will notice that your abstraction is leaking, explain why, and suggest a restructure that fixes the root cause. Its unsolicited observations tend to be higher signal.

**Where it falls short.** Cost and latency. Opus is expensive and slow compared to Sonnet. For routine tasks — scaffolding, simple refactors, boilerplate — you're burning money without getting proportional value. Reserve it for the problems that actually need it.

## Cursor Composer-1: the focused one

Composer-1 is the IDE-native model built into Cursor's Composer panel. It knows what files you have open, what your recent diffs look like, and what your cursor is pointing at. That context is its edge — it doesn't need you to paste code into a chat window because it already has it.

**Genuinely fast.** Composer-1 is noticeably quicker than most models — responses come back in seconds, not tens of seconds. That speed isn't just a convenience. It's the difference between staying in flow and losing your train of thought. When the model keeps up with how fast you're thinking, you stay in the zone. You don't context-switch, you don't check your phone, you don't lose the thread of what you were doing. For the kind of rapid, incremental work that makes up most of a coding day, this matters more than raw capability.

**Tight scope by design.** Composer-1 is optimized for targeted edits. Give it a specific task — fix this function, rename this prop across these files, implement this interface — and it executes cleanly without wandering. It's the model least likely to touch things you didn't ask about.

**Fast iteration loop.** Because it's embedded in the editor, the round-trip from prompt to applied diff is fast. You see the change inline, accept or reject hunks, and move on. This tight feedback loop compounds: each small task takes seconds, so you can chain ten of them in the time another model takes to respond once.

**Context-aware without extra work.** You don't have to manually paste in the relevant code. Composer-1 picks up your open tabs, your recent edits, and your project structure automatically. For targeted changes, this removes a lot of friction.

**Where it falls short.** Composer-1 is not built for multi-step autonomous tasks. If you need it to run tests, read the output, fix the failures, and repeat — that's not its mode. It's a precise instrument, not an autonomous agent.

## Cursor Composer-1.5: the agentic one

Composer-1.5 is a different kind of model. It doesn't just edit files — it acts. It can run terminal commands, read the output, make more edits based on what it sees, and loop until the task is done. It's the closest thing to having an AI developer who can actually execute a full task end-to-end.

**Multi-step execution.** Give Composer-1.5 a task that spans multiple files and requires verification — "add this feature, make sure the tests pass, fix any type errors" — and it will work through the steps autonomously. It runs the build, reads the errors, fixes them, and reruns. This is qualitatively different from a model that just generates code.

**Self-correcting.** Because it can observe the results of its own actions, Composer-1.5 can catch and fix mistakes that a non-agentic model would leave for you to discover. It sees the TypeScript error, understands it in context, and fixes it — without you having to copy-paste the error back into a prompt.

**Edits across the whole codebase.** Composer-1.5 isn't limited to the files you have open. It can navigate the project, find the relevant files, and make coordinated changes across many of them. For large refactors or feature additions that touch many layers, this is a significant capability.

**Where it falls short.** Autonomy has a cost. Composer-1.5 can go down wrong paths and make a lot of changes before you realize it's off track. The longer you let it run without checkpoints, the harder the diff is to review. The fix is the same as with any agentic model: short task scopes, frequent checkpoints, and explicit constraints about what it can and can't touch.

## How personality affects prompting

Knowing these personalities changes how you prompt:

**For Gemini:** be explicit about what you want. If you want alternatives, ask for them. If you want it to push back on your approach, say so. Don't expect it to read between the lines.

**For GPT:** give it clear specs and let it work. It'll fill in reasonable defaults. Be specific about output format and constraints — it's excellent at following them.

**For Claude Sonnet:** set boundaries. "Only modify the files I specify." "If you notice something outside this task, mention it but don't fix it." Then let its creativity work within those boundaries.

**For Claude Opus:** use it for the hard problems. Architecture reviews, complex debugging, design decisions with long-term implications. Pair it with explicit scope constraints to keep the depth focused where you need it.

**For Cursor Composer-1:** point it at the specific thing you want changed and keep the tasks small. Its speed is the feature — short, precise prompts that come back in seconds are what keep you in flow. Don't fight its nature by giving it big open-ended tasks; save those for Opus or Composer-1.5.

**For Cursor Composer-1.5:** give it a complete task with a clear success condition. "Add X feature, make sure the tests pass" is better than "help me with X". Let it run, but check in at natural breakpoints — after the first pass, before it starts touching tests, before it makes architectural changes.

The right model isn't just the smartest one — it's the one whose personality matches the task. A careful model for production refactors. A creative model for design exploration. A balanced model for everyday shipping. An agentic model for end-to-end task execution.
